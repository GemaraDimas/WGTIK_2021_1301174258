{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning for Text Classification (Task Version)_1301174258.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lfu6XLZfrxJ"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hEPRXDqVfFQ"
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import categorical_accuracy\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFM7XtD6fwnK"
      },
      "source": [
        "# Preprocessing the data\n",
        "You already learned that we have to tokenize the text before we can feed it into a neural network. This tokenization process will also remove some of the features of the original text, such as all punctuation or words that are less common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUmJuXFNLo5e"
      },
      "source": [
        "**Below is the AG News Dataset, which contains 120,000 dataset and 4 classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLJjQkjRHvfd"
      },
      "source": [
        "dataset = tfds.load('ag_news_subset', split='train', shuffle_files=True, download=False)\n",
        "\n",
        "texts, target = [], []\n",
        "for example in dataset:\n",
        "  texts.append(example[\"description\"].numpy().decode('utf-8'))\n",
        "  target.append(example[\"label\"].numpy())\n",
        "\n",
        "target_names = ['world','sports','business','sci/tech']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYpYyZ5iVqH4"
      },
      "source": [
        "print (target[:10])\n",
        "\n",
        "print (len(texts))\n",
        "print (len(target))\n",
        "print (len(texts[0].split()))\n",
        "print (texts[0])\n",
        "print (target[0])\n",
        "print (target_names[target[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ymzR5_gSWp"
      },
      "source": [
        "Remember we have to specify the size of our vocabulary. Words that are less frequent will get removed. In this case we want to retain the **10,000** most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBTD2yxWWhuW"
      },
      "source": [
        "vocab_size = ??? #define the vocabulary size\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
        "tokenizer.fit_on_texts(texts) #fitting the tokenizer on the data\n",
        "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntT3kdOiWkYs"
      },
      "source": [
        "print (tokenizer.texts_to_sequences(['Hello King, how are you?']))\n",
        "\n",
        "print (len(sequences))\n",
        "print (len(sequences[0]))\n",
        "print (sequences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtrKOp1xWpix"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {:,} unique words.'.format(len(word_index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykeKKvSbgY1Y"
      },
      "source": [
        "Our text is now converted to sequences of numbers. It makes sense to convert some of those sequences back into text to check what the tokenization did to our text. To this end we create an inverse index that maps numbers to words while the tokenizer maps words to numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2EmQGmYWrqL"
      },
      "source": [
        "# Create inverse index mapping numbers to words\n",
        "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "# Print out text again\n",
        "for w in sequences[0]:\n",
        "    x = inv_index.get(w)\n",
        "    print(x,end = ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBNHmDpBf0H_"
      },
      "source": [
        "# Measuring text length\n",
        "Let's ensure all sequences have the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ8W0xMhWtKB"
      },
      "source": [
        "# Get the average length of a text\n",
        "avg = sum(map(len, sequences)) / len(sequences)\n",
        "\n",
        "# Get the standard deviation of the sequence length\n",
        "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
        "\n",
        "avg,std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy156ffJgmgM"
      },
      "source": [
        "You can see, the average text is about 31 words long. However, the standard deviation is quite small which indicates that some texts are much shorter. We will restrict sequence length to 30 words. You should try out some different sequence lengths and experiment with processing time and accuracy gains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdmeYEG9WyuB"
      },
      "source": [
        "print(pad_sequences([[1,2,3]], maxlen=5))\n",
        "print(pad_sequences([[1,2,3,4,5,6]], maxlen=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKsVc5t9W1Ek"
      },
      "source": [
        "max_length = 30 #set the maximum length of the each data\n",
        "data = pad_sequences(sequences, maxlen=max_length) #padding each data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8Nn9WRf5sV"
      },
      "source": [
        "# Turning labels into One-Hot encodings\n",
        "Labels can quickly be encoded into one-hot vectors with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrdJ_9CTW6aF"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "labels = to_categorical(np.asarray(target))\n",
        "print('Shape of data:', data.shape)\n",
        "print('Shape of labels:', labels.shape)\n",
        "\n",
        "print (target[0])\n",
        "print (labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4631NhFf7Z4"
      },
      "source": [
        "# Split dataset into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reS_GKUPW76h"
      },
      "source": [
        "train_size = int(len(data) * .8) #set training data size\n",
        "xtrain = data[:train_size]\n",
        "ytrain = labels[:train_size]\n",
        "\n",
        "xtest = data[train_size:]\n",
        "ytest = labels[train_size:]\n",
        "\n",
        "xtest_texts = texts[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900biRGbgCLW"
      },
      "source": [
        "# Create Model (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xdQrWsmZ1hd"
      },
      "source": [
        "modelNN = Sequential()\n",
        "modelNN.add(Input(shape=(max_length,)))\n",
        "modelNN.add(Activation('relu'))\n",
        "modelNN.add(Dense(128, activation='sigmoid'))\n",
        "modelNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqupyH-9Z5ax"
      },
      "source": [
        "modelNN.compile(optimizer='adam',\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=[categorical_accuracy])\n",
        "\n",
        "histNN = modelNN.fit(xtrain,ytrain,validation_split=0.2,epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEvnTmrdgD2k"
      },
      "source": [
        "# Create Model (Simple RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGgTRLytXQ44"
      },
      "source": [
        "modelRNN = Sequential()\n",
        "modelRNN.add(Embedding(input_dim=vocab_size,\n",
        "                       output_dim=64,\n",
        "                       input_length,\n",
        "                       trainable=True))\n",
        "modelRNN.add(SimpleRNN(64))\n",
        "modelRNN.add(Dense(20, activation='softmax'))\n",
        "modelRNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAwFCrupXeVt"
      },
      "source": [
        "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
        "modelRNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
        "\n",
        "histRNN = modelRNN.fit(xtrain, ytrain, validation_split=0.2, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_3xMISOn4l9"
      },
      "source": [
        "# Create Model (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmp0dc3Uk3qt"
      },
      "source": [
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3C00Obhk53T"
      },
      "source": [
        "modelLSTM = LSTM(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo0-6PWUlG9b"
      },
      "source": [
        "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
        "modelLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
        "\n",
        "histLSTM = modelLSTM.fit(xtrain, ytrain, validation_split=0.2, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRTkrMzggGKU"
      },
      "source": [
        "# Create Model (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56-7KPpSX0CA"
      },
      "source": [
        "modelCNN = Sequential()\n",
        "modelCNN.add(Embedding(input_dim=vocab_size,\n",
        "                       output_dim=64,\n",
        "                       input_length=max_length,\n",
        "                       trainable=True))\n",
        "modelCNN.add(ConvID(64,2, activation='relu'))\n",
        "modelCNN.add(MaxPoolingID(2))\n",
        "modelCNN.add(ConvID(64,2, activation='relu'))\n",
        "modelCNN.add(MaxPoolingID(2))\n",
        "modelCNN.add(Flatten)\n",
        "modelCNN.add(Dense(64, activation='relu'))\n",
        "modelCNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OraTretaJYq",
        "outputId": "cb05fbdb-fc6f-4624-8030-66106386cce9"
      },
      "source": [
        "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
        "modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
        "\n",
        "histCNN = modelCNN.fit(xtrain, ytrain, validation_split=0.2, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2400/2400 [==============================] - 28s 11ms/step - loss: 0.7351 - categorical_accuracy: 0.6919 - val_loss: 0.3908 - val_categorical_accuracy: 0.8670\n",
            "Epoch 2/2\n",
            "2400/2400 [==============================] - 27s 11ms/step - loss: 0.2854 - categorical_accuracy: 0.9046 - val_loss: 0.3744 - val_categorical_accuracy: 0.8729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNNntQmpgyFw"
      },
      "source": [
        "Our model achieves 66% accuracy on the validation set. Systems like these can be used to assign emails in customer support centers, suggest responses, or classify other forms of text like invoices which need to be assigned to an department. Let's take a look at how our model classified one of the texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEWX8oOgAABX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(histNN, \"categorical_accuracy\")\n",
        "plot_graphs(histNN, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26vrcx0PgJsN"
      },
      "source": [
        "# Example Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeTojVepe1yF"
      },
      "source": [
        "example = xtest[1000] # get the tokens\n",
        "print (xtest_texts[1000])\n",
        "\n",
        "# Print tokens as text\n",
        "for w in example:\n",
        "    x = inv_index.get(w)\n",
        "    print(x,end = ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-zCjoxbaLrX"
      },
      "source": [
        "# Get prediction\n",
        "pred = modelCNN.predict(example.reshape(1,30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK6m8IUMdTZu"
      },
      "source": [
        "# Output predicted category\n",
        "target_names[np.argmax(pred)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}